import streamlit as st
import pandas as pd
import pymongo
import numpy as np
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from joblib import dump, load
import scipy.sparse as sp
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import textstat

st.markdown("""# Sahte Yorum Tespidi

Ne kullandÄ±m:
- NLP: NLTK, TF-IDF, Metin Analizi
- Model: `LogisticRegression`
- Data: `Kaggle` and `Web Scraping`
""")

# NLTK veri setlerini indir (sadece ilk Ã§alÄ±ÅŸtÄ±rmada gerekli)
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)


# Metin iÅŸleme fonksiyonlarÄ±
def clean_text(text):
    if pd.isna(text):
        return ""

    # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = text.lower()

    # HTML etiketlerini temizle
    text = re.sub(r'<[^>]+>', '', text)

    # URL'leri temizle
    text = re.sub(r'http\S+', '', text)

    # Noktalama iÅŸaretlerini temizle
    text = text.translate(str.maketrans('', '', string.punctuation))

    # SayÄ±larÄ± temizle
    text = re.sub(r'\d+', '', text)

    # Fazla boÅŸluklarÄ± temizle
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    # Tokenize
    tokens = word_tokenize(text)

    # Stop words'leri kaldÄ±r ve lemmatize et
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]

    return ' '.join(tokens)


def extract_text_features(text):
    # Metin Ã¶zelliklerini Ã§Ä±kar
    cleaned = clean_text(text)
    processed = preprocess_text(cleaned)

    features = {
        'text_length': len(processed),
        'word_count': len(processed.split()) if processed else 0,
        'avg_word_length': np.mean([len(word) for word in processed.split()]) if processed else 0,
        'exclamation_count': text.count('!'),
        'question_count': text.count('?'),
        'capital_count': sum(1 for c in text if c.isupper()),
        'flesch_score': textstat.flesch_reading_ease(text) if text else 0,
        'flesch_kincaid': textstat.flesch_kincaid_grade(text) if text else 0
    }

    return processed, features


@st.cache_resource
def load_model_and_components():
    """Model ve gerekli bileÅŸenleri yÃ¼kle"""
    try:
        model = load('model.pkl')
        tfidf = load('tfidf.pkl')
        scaler = load('scaler.pkl')
        return model, tfidf, scaler
    except FileNotFoundError:
        st.error("Model dosyalarÄ± bulunamadÄ±! LÃ¼tfen Ã¶nce modeli eÄŸitin ve kaydedin.")
        return None, None, None


def predict_fake_review(text, model, tfidf, scaler):
    """Sahte yorum tahminini yap"""
    try:
        # Metin iÅŸleme
        processed_text, features = extract_text_features(text)

        # TF-IDF vektÃ¶rÃ¼
        tfidf_vector = tfidf.transform([processed_text])

        # SayÄ±sal Ã¶zellikleri hazÄ±rla
        numerical_features = ['text_length', 'word_count', 'avg_word_length',
                              'exclamation_count', 'question_count', 'capital_count',
                              'flesch_score', 'flesch_kincaid']

        numerical_vector = scaler.transform([[features[col] for col in numerical_features]])

        # Ã–zellikleri birleÅŸtir
        X_new = sp.hstack([tfidf_vector, numerical_vector])

        # Tahmin yap
        prediction = model.predict(X_new)[0]
        probability = model.predict_proba(X_new)[0]

        return {
            'prediction': 'Sahte' if prediction == 1 else 'GerÃ§ek',
            'fake_probability': probability[1],
            'real_probability': probability[0]
        }
    except Exception as e:
        st.error(f"Tahmin sÄ±rasÄ±nda hata: {str(e)}")
        return None


# Streamlit arayÃ¼zÃ¼
review = st.text_area("Yorumunuzu girin:", key="comment", height=100)

if st.button("Analiz Et"):
    if review.strip():
        # Model ve bileÅŸenleri yÃ¼kle
        model, tfidf, scaler = load_model_and_components()

        if model and tfidf and scaler:
            # Tahmin yap
            with st.spinner('Yorum analiz ediliyor...'):
                result = predict_fake_review(review, model, tfidf, scaler)

            if result:
                # SonuÃ§larÄ± gÃ¶ster
                st.subheader("Analiz Sonucu:")

                col1, col2 = st.columns(2)

                with col1:
                    st.metric("Tahmin", result['prediction'])

                with col2:
                    confidence = max(result['fake_probability'], result['real_probability'])
                    st.metric("GÃ¼ven OranÄ±", f"{confidence:.2%}")

                # DetaylÄ± probabilities
                st.subheader("DetaylÄ± SonuÃ§lar:")
                st.write(f"ðŸ”´ Sahte olma olasÄ±lÄ±ÄŸÄ±: **{result['fake_probability']:.2%}**")
                st.write(f"ðŸŸ¢ GerÃ§ek olma olasÄ±lÄ±ÄŸÄ±: **{result['real_probability']:.2%}**")

                # GÃ¶rsel gÃ¶sterim
                import matplotlib.pyplot as plt

                fig, ax = plt.subplots(figsize=(8, 4))
                categories = ['GerÃ§ek', 'Sahte']
                probabilities = [result['real_probability'], result['fake_probability']]
                colors = ['green', 'red']

                bars = ax.bar(categories, probabilities, color=colors, alpha=0.7)
                ax.set_ylabel('OlasÄ±lÄ±k')
                ax.set_title('Yorum Analiz Sonucu')
                ax.set_ylim(0, 1)

                # Bar Ã¼zerinde deÄŸerleri gÃ¶ster
                for bar, prob in zip(bars, probabilities):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                            f'{prob:.2%}', ha='center', va='bottom')

                st.pyplot(fig)

    else:
        st.warning("LÃ¼tfen bir yorum girin.")

st.checkbox("MongoDB'ye kaydet (Herkes gÃ¶rebilir)")

# Bilgi kutusu
with st.expander("NasÄ±l Ã§alÄ±ÅŸÄ±r?"):
    st.write("""
    Bu uygulama, yorumlarÄ±n sahte mi gerÃ§ek mi olduÄŸunu belirlemek iÃ§in:

    1. **Metin temizleme**: Yorumlar temizlenir ve standardize edilir
    2. **Ã–zellik Ã§Ä±karÄ±mÄ±**: TF-IDF, metin uzunluÄŸu, noktalama iÅŸaretleri gibi Ã¶zellikler Ã§Ä±karÄ±lÄ±r
    3. **Makine Ã¶ÄŸrenmesi**: EÄŸitilmiÅŸ model ile tahmin yapÄ±lÄ±r
    4. **SonuÃ§**: Sahte/gerÃ§ek tahmini ve gÃ¼ven oranÄ± gÃ¶sterilir
    """)